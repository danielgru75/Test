{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:\n",
    "\n",
    "Modeling Challenge\n",
    "#Python Coding and Data Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the data file and header file provided\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "with open(\"field_names.txt\",\"r\") as f:\n",
    "     field_names= f.read().splitlines()\n",
    "with open('breast-cancer.csv',newline='') as f:\n",
    "    r = csv.reader(f)\n",
    "    data = [line for line in r]\n",
    "with open('breast-cancer.csv','w',newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(field_names)\n",
    "    w.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataframe does not currently have a header, load in the header file and attach it to the dataframe\n",
    "\n",
    "data = pd.read_csv('breast-cancer.csv')\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the mean and median smoothness and compactness for benign and malignant tumors - do they differ? Explain how you would identify this.\n",
    "# Mean smoothness for Mallignant cells\n",
    "print('Mean smoothness for Mallignant cells -',np.mean(data[data['diagnosis']=='M']['smoothness_mean']))\n",
    "\n",
    "# Median smoothnes for Mallignant cells\n",
    "print('Median smoothness for Mallignant cells -', np.median(data[data['diagnosis']=='M']['smoothness_mean']))\n",
    "\n",
    "# Mean smoothness for Benign cells\n",
    "print('Mean smoothness for Benign cells -',np.mean(data[data['diagnosis']=='B']['smoothness_mean']))\n",
    "\n",
    "# Median smoothnes for Benign cells\n",
    "print('Median smoothnes for Benign cells -',np.median(data[data['diagnosis']=='B']['smoothness_mean']))\n",
    "\n",
    "# Mean compactness for Mallignant cells\n",
    "print('Mean compactness for Mallignant cells -',np.mean(data[data['diagnosis']=='M']['compactness_mean']))\n",
    "\n",
    "# Median compactness for Mallignant cells\n",
    "print('Median compactness for Mallignant cells -', np.median(data[data['diagnosis']=='M']['compactness_mean']))\n",
    "\n",
    "# Mean compactness for Benign cells\n",
    "print('Mean compactness for Benign cells -',np.mean(data[data['diagnosis']=='B']['compactness_mean']))\n",
    "\n",
    "# Median compactness for Benign cells\n",
    "print('Median compactness for Benign cells -',np.median(data[data['diagnosis']=='B']['compactness_mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function to generate bootstrap samples of the data.\n",
    "\n",
    "def sample_data(size, dataset):\n",
    "    sample_data = dataset.values[np.random.randint(size, size=size)]\n",
    "    return sample_data\n",
    "\n",
    "#example\n",
    "s_data = sample_data(30,data)\n",
    "\n",
    "print('Size of sampled data',s_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify 2-3 variables that are predictive of a malignant tumor.\n",
    "#Display the relationship visually and write 1-2 sentences explaining the relationship.\n",
    "# encoding 'diagnosis' feature as 0/1\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "le = LabelEncoder()\n",
    "le.fit(data['diagnosis'])\n",
    "data['diagnosis']=le.transform(data['diagnosis'])\n",
    "\n",
    "# print top 4 correlation coefficients\n",
    "for i in sorted(corr_numeric[\"diagnosis\"])[-5:]:\n",
    "    if i != 1.0:\n",
    "        print(corr_numeric[corr_numeric[\"diagnosis\"] == i].index.values[0])\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "sns.pairplot(data, x_vars=['diagnosis'], y_vars=['perimeter_sd_error','concavity_worst','concave_points_sd_error','fractal_dimension_mean'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "corr_numeric = data.corr()\n",
    "\n",
    "sns.heatmap(corr_numeric, cbar=True, cmap=\"RdBu_r\")\n",
    "plt.title(\"Correlation Matrix\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling\n",
    "#Build a model to predict the malignant tumors.\n",
    "#Use at least two classification techniques; compare and contrast the advantages and disadvantages of each.\n",
    "#Identify how you would control for overfitting in each classification technique.\n",
    "#Evaluate the performance of each model.\n",
    "#In each model, identify the most important predictive variables and explain how you identified them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = data.loc[: , data.columns != 'diagnosis']\n",
    "print(X.shape)\n",
    "\n",
    "Y = data['diagnosis']\n",
    "print(Y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=5)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# checking uniform distribution of y labels\n",
    "print(y_train.value_counts(), y_test.value_counts())\n",
    "\n",
    "\n",
    "# as we had seen there is strong correlations between predicted variables and target variables. \n",
    "# Logistic Reg won't help us solving this problem\n",
    "# dataset violates LR assumptions\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#random state to allow reproducibility of results\n",
    "LR_model= LogisticRegression(random_state=0)\n",
    "LR_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LR_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "# checking if model overfits\n",
    "\n",
    "y_pred_train = LR_model.predict(X_train)\n",
    "\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "print(confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print('Train acc -',accuracy_score(y_train, y_train_pred))\n",
    "print('Train conf matrix \\n',confusion_matrix(y_train, y_train_pred), '\\n-----')\n",
    "\n",
    "# Testing \n",
    "y_pred = clf.predict(X_test)\n",
    "print('Train acc -', accuracy_score(y_test,y_pred))\n",
    "# test accuracy < train accuracy (good to go)\n",
    "print('Train conf matrix \\n', confusion_matrix(y_test, y_pred), '\\n-----')\n",
    "\n",
    "# Control for overfitting\n",
    "# Cross validation performed for more rigorous analysis (check against overfitting)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, X, Y, cv=3)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "# checking top 3 highest contributing variables\n",
    "\n",
    "ind_top = clf.feature_importances_.argsort()[-3:][::-1]\n",
    "print('Top 3 contributing variables are -- ')\n",
    "for i in ind_top:\n",
    "    print(X_train.columns[i])\n",
    "\n",
    "# Note - this result is line with our EDA above for correaltion diagram\n",
    "# Top contributing varibales are same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBM\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GBC_clf = GradientBoostingClassifier(random_state=0)\n",
    "GBC_clf.fit(X_train,y_train)\n",
    "y_train_pred = GBC_clf.predict(X_train)\n",
    "print('Training acc -',accuracy_score(y_train,y_train_pred))\n",
    "print('Train conf matrix \\n',confusion_matrix(y_train,y_train_pred), '\\n-----')\n",
    "\n",
    "# no false positives/negatives -> indicating overfitting\n",
    "# additionally 1 classificaito score also indicating same\n",
    "\n",
    "y_test_pred = GBC_clf.predict(X_test)\n",
    "print('Test acc -',accuracy_score(y_test, y_test_pred))\n",
    "print('Test conf matrix \\n',confusion_matrix(y_test, y_test_pred), '\\n-----')\n",
    "\n",
    "# Control for overfitting\n",
    "# Cross validation performed for more rigorous analysis (check against overfitting)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(GBC_clf, X, Y, cv=4)\n",
    "print(\"CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking top 3 highest contributing variables\n",
    "\n",
    "ind_top = GBC_clf.feature_importances_.argsort()[-3:][::-1]\n",
    "print('Top 3 contributing variables are -- ')\n",
    "for i in ind_top:\n",
    "    print(X_train.columns[i])\n",
    "    \n",
    "# pretty much inline with our above analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "To Technical Audiences\n",
    "\n",
    "Explain the limitations of your analysis and identify possible further steps you could take.\n",
    "\n",
    "However please understand that the model should be further developed ( this is a short version) but in normal circumstances I would have used Tensorflow or PyTorch using a neural network and not simply Random Forest or GBM. However for these limited purposes these should do ( for now). On a TF network a similar model I built obtained 97% accuracy.\n",
    "\n",
    "There is also a need for the correlation factors as well as F score . Accuracy might be misleading in this case to use as a single performance criteria. ROC might came into play as well. \n",
    "\n",
    "\n",
    "To Non-Technical Audiences\n",
    "\n",
    "\n",
    "The dataset consists of 569 medical observations with 212 Malign datarows so the data set is balanced and needs no augmentation or recalibration. \n",
    "\n",
    "The dataset altough of certain quality ( no NaN ) is headerless so we had to apply the heading corresponding to each feature and to the label.\n",
    "\n",
    "We identified 4 major features which are driving a high corelation to the target diagnosis:perimeter_sd_error','concavity_worst','concave_points_sd_error','fractal_dimension_mean'. These have shown a large coreelation to the diagnosis. \n",
    "\n",
    "This modelling is binary(benign /malign) therefore as first step of our analysis the accuracy score was used for both models: over 90% result. \n",
    " The dataset is small in nature and prone to overfitting ( i.e. following too close the diagnosis). This is a major limitation of the analysis and bigger dataset should be provided. \n",
    " \n",
    "\n",
    "Write a short summary of your analysis, explaining how your model works and how it performs.\n",
    "\n",
    "The two models \n",
    "\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]:587â€“588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.[4]\n",
    "\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\n",
    "\n",
    "Briefly explain the factors that contributed to malignant vs benign tumor identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# student-sample-1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load data\n",
    "d = pd.read_csv('../data/train.csv')\n",
    "\n",
    "\n",
    "# Setup data for prediction\n",
    "\n",
    "# incorrect reference of dataframe \"data\". Correct reference is 'd'\n",
    "#x1 = data.SalaryNormalized\n",
    "x1 = d.SalaryNormalized\n",
    "#same here\n",
    "#x2 = pd.get_dummies(data.ContractType)\n",
    "y = pd.get_dummies(d.ContractType)\n",
    "#you should use y for labels ( x for features)\n",
    "\n",
    "# Setup model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Evaluate model. Your model is completely empty. \n",
    "# Please re-access your model requirements i.e.\n",
    "# what is it that you want to predict i.e. target variable (Y)\n",
    "# identify your inputs i.e. predictor variables (X)\n",
    "# using above variable for your model is a bad design choice. Will give very poor results.\n",
    "\n",
    "# incorrect import statement \n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# using cv=1, beats the purpose of k-fold cross validation\n",
    "# when cv=1, it effectively becomes a simple test-train split.\n",
    "# Which doesn't help in reducing overfitting\n",
    "\n",
    "# Some changes in attribute order for cross_val_score\n",
    "# The correct format for cross_val_score is cross_val_score(estimator, X, y=None....)\n",
    "# updating attribute names x1=>x , x2=>Y\n",
    "# 'scoring' parameter doesn't define the type of error metric i.e. incorrect attribute value\n",
    "\n",
    "#scores = cross_val_score(model, x2, x1, cv=1, scoring='mean_absolute_error')\n",
    "\n",
    "## correct format --\n",
    "scores = cross_val_score(model, x1, y, cv=5)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# student-sample-2.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "\n",
    "# Setup data for prediction\n",
    "y = data.SalaryNormalized\n",
    "X = pd.get_dummies(data.ContractType)\n",
    "\n",
    "# Setup model\n",
    "model = LinearRegression()\n",
    "# Evaluate model\n",
    "\n",
    "# Please re-access your model requirements i.e.\n",
    "# what is it that you want to predict i.e. target variable (Y)\n",
    "# identify your inputs i.e. predictor variables (X)\n",
    "# using above variable for your model is a bad design choice. Will give very poor results.\n",
    "\n",
    "# Some changes in attribute order for cross_val_score\n",
    "# The correct format for cross_val_score is cross_val_score(estimator, X, y=None....)\n",
    "# updating attribute names x1=>x , x2=>Y\n",
    "# 'scoring' parameter doesn't define the type of error metric i.e. incorrect attribute value\n",
    "\n",
    "# Evaluate model\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='mean_absolute_error')\n",
    "# you can read more about cross_val_score on sklearn website https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "## correct format --\n",
    "## scores = cross_val_score(model, x1, y, cv=5)\n",
    "print(scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
